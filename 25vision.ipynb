{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WVU-OUIajiOlVZhUlkSiY3wsjVwo8ZgN",
      "authorship_tag": "ABX9TyM4Yq2Dqug+vPtbxfDSAtNb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "dVgJB4gUchbS",
        "outputId": "6dabdc6d-350b-43d3-9eaa-04f7afdcb6e7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'scikitplot'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-68566011.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatthews_corrcoef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscikitplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_roc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scikitplot'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import random\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers, Sequential\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
        "from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay\n",
        "# Install scikitplot if not already installed\n",
        "!pip install scikitplot\n",
        "from scikitplot.metrics import plot_roc\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Context-Free Grammar (CFG):\n",
        "class CFG:\n",
        "    EPOCHS= 50\n",
        "    BATCH_SIZE= 64\n",
        "    SEED= 42\n",
        "    TF_SEED= 768\n",
        "    HEIGHT= 224\n",
        "    WIDTH= 224\n",
        "    CHANNELS= 3\n",
        "    IMAGE_SIZE=(224,224,3)"
      ],
      "metadata": {
        "id": "rTzB9l3Fcwag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH=\"../input/chest-xray-covid19-pneumonia/Data/\"\n",
        "TRAIN_PATH=  \"../input/chest-xray-covid19-pneumonia/Data/train/\"\n",
        "TEST_PATH= \"../input/chest-xray-covid19-pneumonia/Data/test/\"\n",
        "\n"
      ],
      "metadata": {
        "id": "TB1jIuMpc_LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate a summary of dataset:\n",
        "\n",
        "print(\"DATASET SUMMARY\")\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(DATASET_PATH):\n",
        "    print(f'there are {len(dirnames)} directories and {len(filenames)} images in {dirpath}')\n",
        "print('\\n---------------------------------------')"
      ],
      "metadata": {
        "id": "_zDh-N9mdGGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images= glob.glob(f'{TRAIN_PATH}**/*.jpg')\n",
        "test_images= glob.glob(f'{TEST_PATH}**/*.jpg')"
      ],
      "metadata": {
        "id": "EUaswFzGdMYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size= len(train_images)\n",
        "test_size= len(test_images)\n",
        "\n",
        "total= train_size+test_size\n",
        "print(f'Total: {total}')"
      ],
      "metadata": {
        "id": "XtR5U1wrdNTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_labels(image_paths):\n",
        "    return [_.split('/')[-2:][0] for _ in image_paths]\n",
        "def build_df(image_paths, labels):\n",
        "    df= pd.DataFrame({\n",
        "        'image_path': image_paths,\n",
        "        'label': generate_labels(labels)\n",
        "    })\n",
        "\n",
        "    df['label_encoded'] = df.apply(lambda row: 0 if row.label == 'COVID19' else 1 if row.label == 'NORMAL' else 2, axis=1)\n",
        "\n",
        "\n",
        "    return df.sample(frac=1, random_state= CFG.SEED).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "A9NVH6IKdRWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Build the datafram\n",
        "\n",
        "train_df= build_df(train_images, generate_labels(train_images))\n",
        "test_df= build_df(test_images, generate_labels(test_images))\n"
      ],
      "metadata": {
        "id": "s25eTln8dzK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load(image_path):\n",
        "    # Read the image from the file\n",
        "    image = tf.io.read_file(image_path)\n",
        "    # Decode the image to a uint8 tensor\n",
        "    image = tf.io.decode_image(image, channels=3)\n",
        "    # Resize the image\n",
        "    image = tf.image.resize(image, [CFG.HEIGHT, CFG.WIDTH], method=tf.image.ResizeMethod.LANCZOS3)\n",
        "    return image"
      ],
      "metadata": {
        "id": "7CTLCsPWd132"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def view_sample(image, label, color_map='rgb',fig_size=(6,4)):\n",
        "    plt.figure(figsize=fig_size)\n",
        "    if color_map== 'rgb':\n",
        "        plt.imshow(image)\n",
        "    else:\n",
        "        plt.imshow(tf.image.rgb_to_grayscale(image), cmap= color_map)\n",
        "    plt.title(f'label:{label}', fontsize= 16)\n",
        "    return"
      ],
      "metadata": {
        "id": "QJkQJKaTjQFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select random sample from train_df\n",
        "idx= random.sample(train_df.index.to_list(),1)[0]\n",
        "#load the random sample and label\n",
        "#sample_image, sample_label= _load(train_df.image_path[idx])\n",
        "sample_image = _load(train_df.image_path[idx])\n",
        "sample_label = train_df.label_encoded[idx]\n",
        "\n",
        "#view the random sample colormap= gray\n",
        "view_sample(sample_image, sample_label, color_map='gray')"
      ],
      "metadata": {
        "id": "ZrddWexFjTz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View multiple samples:\n",
        "def view_multiple_samples(df,sample_loader, count=10, color_map='rgb', fig_size=(14,10)):\n",
        "    rows= count//5\n",
        "    if rows%5>0:\n",
        "        rows+=1\n",
        "    idx= random.sample(df.index.to_list(), count)\n",
        "    fig= plt.figure(figsize=fig_size)\n",
        "\n",
        "    for colum,_ in enumerate(idx):\n",
        "        plt.subplot(rows,5,colum+1)\n",
        "        plt.title(f'Label: {df.label[_]}')\n",
        "        if color_map=='rgb':\n",
        "            plt.imshow(sample_loader(df.image_path[_]))\n",
        "        else:\n",
        "            plt.imshow(tf.image.rgb_to_grayscale(sample_loader(df.image_path[_])), cmap=color_map)\n",
        "    return\n"
      ],
      "metadata": {
        "id": "m_wpNM1fjU06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view_multiple_samples(train_df, _load, count=15, color_map='inferno', fig_size=(15,20))"
      ],
      "metadata": {
        "id": "XXEnguN8jaDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create train/ val split with training\n",
        "train_split_idx, val_split_idx,_,_= train_test_split(train_df.index,\n",
        "                                                    train_df.label_encoded,\n",
        "                                                    test_size= 0.15,\n",
        "                                                    stratify=train_df.label_encoded,\n",
        "                                                    random_state= CFG.SEED)"
      ],
      "metadata": {
        "id": "349F_Dfkjes-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get new training and validation data\n",
        "\n",
        "train_new_df= train_df.iloc[train_split_idx].reset_index(drop=True)\n",
        "val_df= train_df.iloc[val_split_idx].reset_index(drop=True)\n",
        "#view shape\n",
        "train_new_df.shape, val_df.shape\n",
        "train_new_df"
      ],
      "metadata": {
        "id": "skMT-g55jhqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "build augmentation layeer"
      ],
      "metadata": {
        "id": "3srm61adjlOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Augmentation layer\n",
        "augmentation_layer= Sequential([\n",
        "    layers.RandomFlip(mode='horizontal_and_vertical',seed=CFG.TF_SEED),\n",
        "    layers.RandomZoom(height_factor=(-.01,0.1),width_factor=(-0.1,0.1),seed=CFG.TF_SEED)\n",
        "],name= 'augmentation_layer')"
      ],
      "metadata": {
        "id": "6ifEjb0QjoDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show inferno Augmented Image\n",
        "\n",
        "image= tf.image.rgb_to_grayscale(sample_image)\n",
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(8,6))\n",
        "#set spacing between subplot\n",
        "fig.tight_layout(pad=6.0)\n",
        "#view orginal image\n",
        "ax1.set_title('Orginal image',fontsize=15)\n",
        "ax1.imshow(image,cmap='inferno')\n",
        "#view augmented image\n",
        "ax2.set_title('Augmented image',fontsize=15)\n",
        "ax2.imshow(augmentation_layer(image),cmap='inferno')"
      ],
      "metadata": {
        "id": "yShBC6kFjrWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Gray Augmented Image\n",
        "\n",
        "image= tf.image.rgb_to_grayscale(sample_image)\n",
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(8,6))\n",
        "#set spacing between subplot\n",
        "fig.tight_layout(pad=6.0)\n",
        "#view orginal image\n",
        "ax1.set_title('Orginal image',fontsize=15)\n",
        "ax1.imshow(image,cmap='gray')\n",
        "#view augmented image\n",
        "ax2.set_title('Augmented image',fontsize=15)\n",
        "ax2.imshow(augmentation_layer(image),cmap='gray')"
      ],
      "metadata": {
        "id": "za-oRJx9jvTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_new_df"
      ],
      "metadata": {
        "id": "lMhphOAnjycs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_labels(labels,encode_depth=3):\n",
        "    return tf.one_hot(labels,depth=encode_depth).numpy()\n",
        "def create_pipeline(df,load_function,augment=False,batch_size=32,shuffle=False,cache=None,prefetch=False):\n",
        "    #get image path and labels from data_frame\n",
        "    image_paths= df.image_path\n",
        "    image_labels= encode_labels(df.label_encoded)\n",
        "    AUTOTUNE= tf.data.AUTOTUNE\n",
        "    #Create dataset with raw data from data frame\n",
        "    ds= tf.data.Dataset.from_tensor_slices((image_paths,image_labels))\n",
        "    #map augmentation layer and load function to dataset input if augment is true\n",
        "    if augment:\n",
        "        ds=ds.map(lambda x,y: (augmentation_layer(load_function(x)),y),num_parallel_calls= AUTOTUNE)\n",
        "    else:\n",
        "        ds= ds.map(lambda x,y: (load_function(x),y),num_parallel_calls= AUTOTUNE)\n",
        "    #Applying shuffing based on condion\n",
        "    if shuffle:\n",
        "        ds= ds.shuffle(buffer_size=1000)\n",
        "    #applying batching\n",
        "    ds= ds.batch(batch_size)\n",
        "    #applying caching based on condion\n",
        "    if cache!=None:\n",
        "        ds= ds.cache(cache)\n",
        "    if prefetch:\n",
        "        ds= ds.prefetch(buffer_size= AUTOTUNE)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "opWlLz7Aj14C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load(image_path):\n",
        "    # Read the image from the file\n",
        "    image = tf.io.read_file(image_path)\n",
        "    # Decode the image to a uint8 tensor\n",
        "    image = tf.io.decode_image(image, channels=3)\n",
        "    # Ensure the image has a known shape\n",
        "    image.set_shape([CFG.HEIGHT, CFG.WIDTH, 3])\n",
        "    # Resize the image\n",
        "    image = tf.image.resize(image, [CFG.HEIGHT, CFG.WIDTH], method=tf.image.ResizeMethod.LANCZOS3)\n",
        "    return image"
      ],
      "metadata": {
        "id": "Ya_5rwaSj46y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Training Pipeline\n",
        "train_ds= create_pipeline(train_new_df,_load,\n",
        "                         augment= True,\n",
        "                         batch_size= CFG.BATCH_SIZE,\n",
        "                         shuffle=False, prefetch= True)\n",
        "\n"
      ],
      "metadata": {
        "id": "Qwcix1Wvj8sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Test Pipeline\n",
        "test_ds= create_pipeline(test_df,_load,\n",
        "                         batch_size= CFG.BATCH_SIZE,\n",
        "                         shuffle=False, prefetch= False)"
      ],
      "metadata": {
        "id": "mWRltY4Uj_tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Validation Pipeline\n",
        "val_ds= create_pipeline(val_df,_load,\n",
        "\n",
        "                         batch_size= CFG.BATCH_SIZE,\n",
        "                         shuffle=False, prefetch= False)"
      ],
      "metadata": {
        "id": "SFngcTM0kCMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "id": "vywWaqLHkEOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-8: Design and Develop Custom CNN Model"
      ],
      "metadata": {
        "id": "UNqAw2oXkH8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_model():\n",
        "    initializer= tf.keras.initializers.GlorotNormal()\n",
        "    cnn_sequential = Sequential([\n",
        "        layers.Input(shape=CFG.IMAGE_SIZE, dtype=tf.float32, name='input_image'),\n",
        "        layers.Conv2D(16,kernel_size=3, activation='relu', kernel_initializer=initializer),\n",
        "        layers.Conv2D(16,kernel_size=3, activation='relu', kernel_initializer=initializer),\n",
        "        layers.MaxPool2D(pool_size=2,padding='valid'),\n",
        "\n",
        "\n",
        "        layers.Conv2D(8,kernel_size=3, activation='relu', kernel_initializer=initializer),\n",
        "        layers.Conv2D(8,kernel_size=3, activation='relu', kernel_initializer=initializer),\n",
        "        layers.MaxPool2D(pool_size=2),\n",
        "\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu', kernel_initializer=initializer),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(3, activation='softmax', kernel_initializer=initializer),\n",
        "    ], name='cnn_sequential_model')\n",
        "    return cnn_sequential\n"
      ],
      "metadata": {
        "id": "_-zHB1S0kJkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_model16():\n",
        "    initializer= tf.keras.initializers.GlorotNormal()\n",
        "    cnn_sequential = Sequential([\n",
        "        layers.Input(shape=CFG.IMAGE_SIZE, dtype=tf.float32, name='input_image'),\n",
        "        layers.Conv2D(16,kernel_size=3, activation='relu', kernel_initializer=initializer),\n",
        "        layers.Conv2D(16,kernel_size=3, activation='relu', kernel_initializer=initializer),\n",
        "        layers.MaxPool2D(pool_size=2,padding='valid'),\n",
        "\n",
        "\n",
        "        layers.Conv2D(8,kernel_size=3, activation='relu', kernel_initializer=initializer),\n",
        "        layers.Conv2D(8,kernel_size=3, activation='relu', kernel_initializer=initializer),\n",
        "        layers.MaxPool2D(pool_size=2),\n",
        "\n",
        "         layers.Conv2D(4,kernel_size=3, activation='relu', kernel_initializer=initializer),\n",
        "        layers.Conv2D(4,kernel_size=3, activation='relu', kernel_initializer=initializer),\n",
        "        layers.MaxPool2D(pool_size=2),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu', kernel_initializer=initializer),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(3, activation='softmax', kernel_initializer=initializer),\n",
        "    ], name='cnn_sequential_model')\n",
        "    return cnn_sequential"
      ],
      "metadata": {
        "id": "uY1E0g4HkQp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the model\n",
        "model_cnn = cnn_model()\n",
        "model_cnn.summary()"
      ],
      "metadata": {
        "id": "yDMXnYf7kTxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, num_epochs, callbacks_list, tf_train_data, tf_valid_data=None, shuffling= True):\n",
        "    model_history={}\n",
        "    if tf_valid_data!=None:\n",
        "        model_history= model.fit(tf_train_data,\n",
        "                                 epochs= num_epochs,\n",
        "                                 validation_data= tf_valid_data,\n",
        "                                 validation_steps= int(len(tf_valid_data)),\n",
        "                                 callbacks= callbacks_list,\n",
        "                                 shuffle= shuffling\n",
        "                                )\n",
        "    if tf_valid_data==None:\n",
        "        model_history= model.fit(tf_train_data,\n",
        "                                 epochs= num_epochs,\n",
        "                                 callbacks= callbacks_list,\n",
        "                                 shuffle= shuffling\n",
        "                                )\n",
        "    return model_history\n"
      ],
      "metadata": {
        "id": "Lo7clIwgkYpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping_callback= tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                         patience=5,\n",
        "                                                         verbose=1,\n",
        "                                                         restore_best_weights=True)\n",
        "\n",
        "reduce_lr_callback= tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                                        patience=3,\n",
        "                                                        factor=0.1,\n",
        "                                                        verbose=1)\n",
        "\n",
        "CALLBACKS=[early_stopping_callback, reduce_lr_callback]\n",
        "METRICS=['accuracy']"
      ],
      "metadata": {
        "id": "_rV41IdKkbuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(CFG.SEED)\n",
        "model_cnn.compile(loss='categorical_crossentropy',\n",
        "                 optimizer= tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
        "                 metrics=METRICS)\n",
        "\n",
        "print(f'training{model_cnn.name}.')\n",
        "print(f'Train On: {len(train_new_df)} samples, validate On: {len(val_df)} samples.')\n",
        "print('----------------------------------------------------------')\n",
        "cnn_history= train_model(model_cnn, CFG.EPOCHS, CALLBACKS, train_ds, val_ds, shuffling=True)\n",
        "\n",
        "model_cnn.save(\"Trained Model/xray.h5\")"
      ],
      "metadata": {
        "id": "UB9WMj0skf9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_evaluatiion= model_cnn.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "6cI9zEb1kjDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "model=load_model('../working/Trained Model/xray.h5')\n"
      ],
      "metadata": {
        "id": "mF_8WIZBkl3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now Predictions for random image"
      ],
      "metadata": {
        "id": "a_1pvcZcko6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "\n",
        "\n",
        "img= image.load_img(\"/kaggle/input/chest-xray-covid19-pneumonia/Data/test/COVID19/COVID19(460).jpg\",target_size=(224,224))\n",
        "\n",
        "x= image.img_to_array(img)\n",
        "x=x/255\n",
        "x= np.expand_dims(x, axis=0)\n",
        "img_data=preprocess_input(x)\n",
        "img_data.shape\n",
        "preds= model.predict(x)\n",
        "preds= np.argmax(preds, axis=1)\n",
        "\n",
        "if preds==0:\n",
        "    preds=\"The image is Covid19\"\n",
        "elif preds==1:\n",
        "    preds=\"The image is NORMAL\"\n",
        "else:\n",
        "    preds=\"The image is Pneumonia\"\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "ebsR8yUOktKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "id": "S6n85MICkvpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_test_probabilities=model_cnn.predict(test_ds, verbose=1)\n",
        "cnn_test_predictions= tf.argmax(cnn_test_probabilities, axis=1)"
      ],
      "metadata": {
        "id": "ZwGTfCCqky4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_test_probabilities"
      ],
      "metadata": {
        "id": "92yWFQP9kzsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-10: Now Plot the Training Loss, Validation Loss, Training Accuracy, Validation Accuracy"
      ],
      "metadata": {
        "id": "N43hBptmk6EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def plot_training_curves(history):\n",
        "    loss= np.array(history.history['loss'])\n",
        "    val_loss= np.array(history.history['val_loss'])\n",
        "\n",
        "    accuracy= np.array(history.history['accuracy'])\n",
        "    val_accuracy= np.array(history.history['val_accuracy'])\n",
        "\n",
        "    epochs= range(len(history.history['loss']))\n",
        "\n",
        "    fig, (ax1, ax2)= plt.subplots(1,2,figsize=(10,3))\n",
        "\n",
        "    #plot loss\n",
        "    ax1.plot(epochs, loss, label='traing_loss', marker='o')\n",
        "\n",
        "    ax1.plot(epochs, val_loss, label='val_loss', marker='o')\n",
        "\n",
        "    ax1.fill_between(epochs,loss, val_loss, where=(loss>val_loss),color='C0',alpha=0.3,interpolate=True)\n",
        "    ax1.fill_between(epochs,loss, val_loss, where=(loss<val_loss),color='C1',alpha=0.3,interpolate=True)\n",
        "\n",
        "    ax1.set_title('Loss(Lower Means Better)',fontsize= 16)\n",
        "    ax1.set_xlabel('Epochs', fontsize=10)\n",
        "\n",
        "    ax1.legend()\n",
        "\n",
        "    #plot Accuracy\n",
        "    ax2.plot(epochs, accuracy, label='traing_accuracy', marker='o')\n",
        "\n",
        "    ax2.plot(epochs, val_accuracy, label='val_accuracy', marker='o')\n",
        "\n",
        "    ax2.fill_between(epochs,accuracy, val_accuracy, where=(accuracy>val_accuracy),color='C0',alpha=0.3,interpolate=True)\n",
        "    ax2.fill_between(epochs,accuracy, val_accuracy, where=(accuracy<val_accuracy),color='C1',alpha=0.3,interpolate=True)\n",
        "\n",
        "    ax2.set_title('Accuracy(Higher Means Better)',fontsize= 16)\n",
        "    ax2.set_xlabel('Epochs', fontsize=10)\n",
        "\n",
        "    ax2.legend()\n"
      ],
      "metadata": {
        "id": "ZpxIXbRck7Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_curves(cnn_history)"
      ],
      "metadata": {
        "id": "CLPRXK8FlBGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}