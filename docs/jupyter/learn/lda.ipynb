{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f4110398",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-11T18:57:21.723251Z",
          "start_time": "2023-09-11T18:57:21.715643Z"
        },
        "id": "f4110398"
      },
      "source": [
        "Linear Discriminant Analysis\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "===\n",
        "\n",
        "Author: Nathan A. Mahynski\n",
        "\n",
        "Date: 2023/09/12\n",
        "\n",
        "Description: Derivation and examples of [LDA](https://en.wikipedia.org/wiki/Linear_discriminant_analysis).  LDA is similar to PCA in that it tries to find the linear combination of features that characterize, or summarize, the data; however, unlike PCA, LDA explicitly attempts to model the differences between classes (supervised) rather than just the \"shape\" of the data (unsupervised) like PCA does.\n",
        "\n",
        "A thorough comparison [discriminant analysis methods](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) can be found at [All Models are Wrong](https://allmodelsarewrong.github.io/discanalysis.html) or Sebastian Raschka's [blog](https://sebastianraschka.com/Articles/2014_python_lda.html). LDA is a supervised method that can be used for classification or dimensionality reduction (often [followed by classification](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)).\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mahynski/pychemauth/blob/main/docs/jupyter/learn/lda.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c420be41",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:02.085638Z",
          "start_time": "2023-09-19T13:49:02.080534Z"
        },
        "id": "c420be41",
        "outputId": "427c6022-6e39-4f66-eab9-8a8e87672b9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/mahynski/pychemauth@main\n",
            "  Cloning https://github.com/mahynski/pychemauth (to revision main) to /tmp/pip-req-build-cepacyhn\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mahynski/pychemauth /tmp/pip-req-build-cepacyhn\n",
            "  Resolved https://github.com/mahynski/pychemauth to commit f95bfbf6e694fc5535cdc4579d290df7a500fc84\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting BorutaShap@ git+https://github.com/Ekeany/Boruta-Shap.git@38af879 (from pychemauth==0.0.0b4)\n",
            "  Cloning https://github.com/Ekeany/Boruta-Shap.git (to revision 38af879) to /tmp/pip-install-919f3a1k/borutashap_3ad4146357ea4b669b8a6e4120ac2d37\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Ekeany/Boruta-Shap.git /tmp/pip-install-919f3a1k/borutashap_3ad4146357ea4b669b8a6e4120ac2d37\n",
            "\u001b[33m  WARNING: Did not find branch or tag '38af879', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running command git checkout -q 38af879\n",
            "  Resolved https://github.com/Ekeany/Boruta-Shap.git to commit 38af879\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kennard-stone==2.2.1 (from pychemauth==0.0.0b4)\n",
            "  Using cached kennard_stone-2.2.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting mypy==1.11.2 (from pychemauth==0.0.0b4)\n",
            "  Using cached mypy-1.11.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting types-requests==2.32.0.20240914 (from pychemauth==0.0.0b4)\n",
            "  Using cached types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting baycomp==1.0.3 (from pychemauth==0.0.0b4)\n",
            "  Using cached baycomp-1.0.3.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: bokeh>=3.4.3 in /usr/local/lib/python3.12/dist-packages (from pychemauth==0.0.0b4) (3.7.3)\n",
            "Collecting bokeh-sampledata==2024.2 (from pychemauth==0.0.0b4)\n",
            "  Using cached bokeh_sampledata-2024.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting imbalanced-learn==0.11.0 (from pychemauth==0.0.0b4)\n",
            "  Using cached imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.12/dist-packages (from pychemauth==0.0.0b4) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (from pychemauth==0.0.0b4) (7.7.1)\n",
            "Collecting matplotlib==3.7.2 (from pychemauth==0.0.0b4)\n",
            "  Using cached matplotlib-3.7.2.tar.gz (38.1 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nodejs==0.1.1 (from pychemauth==0.0.0b4)\n",
            "  Using cached nodejs-0.1.1.tar.gz (2.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.26.4 (from pychemauth==0.0.0b4)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting pandas==2.1.4 (from pychemauth==0.0.0b4)\n",
            "  Using cached pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting pre-commit==3.3.3 (from pychemauth==0.0.0b4)\n",
            "  Using cached pre_commit-3.3.3-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting scikit-learn==1.3.0 (from pychemauth==0.0.0b4)\n",
            "  Using cached scikit-learn-1.3.0.tar.gz (7.5 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scipy==1.11.1 (from pychemauth==0.0.0b4)\n",
            "  Using cached scipy-1.11.1.tar.gz (56.0 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    !pip install git+https://github.com/mahynski/pychemauth@main\n",
        "    import os\n",
        "    os.kill(os.getpid(), 9) # Automatically restart the runtime to reload libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f60adae",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:02.540370Z",
          "start_time": "2023-09-19T13:49:02.089208Z"
        },
        "id": "9f60adae"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import pychemauth\n",
        "except:\n",
        "    raise ImportError(\"pychemauth not installed\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import watermark\n",
        "%load_ext watermark\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a350867e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:03.050581Z",
          "start_time": "2023-09-19T13:49:02.542804Z"
        },
        "id": "a350867e"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import imblearn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from pychemauth.preprocessing.scaling import CorrectedScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b03266b4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:03.076560Z",
          "start_time": "2023-09-19T13:49:03.052465Z"
        },
        "id": "b03266b4"
      },
      "outputs": [],
      "source": [
        "%watermark -t -m -v --iversions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e918e143",
      "metadata": {
        "id": "e918e143"
      },
      "source": [
        "LDA as a classifier\n",
        "---\n",
        "\n",
        "LDA classifiers are [attractive](https://scikit-learn.org/stable/modules/lda_qda.html#linear-and-quadratic-discriminant-analysis) because they:\n",
        "\n",
        "1. Have closed-form solutions that can be easily computed,\n",
        "2. Are inherently multiclass, have proven to work well in practice,\n",
        "3. Have no hyperparameters to tune!\n",
        "\n",
        "However, LDA makes certain strong assumptions when functioning as a classifier:\n",
        "\n",
        "1. The data is normally distributed,\n",
        "2. The features are statistically independent,\n",
        "3. Covariance matrices are identical for every class.\n",
        "\n",
        "Why is that?  LDA can actually be derived from probabilistic models that describe the class conditional distribution of the data, namely $P(y=k|x)$, for class $k$ given $x$.  LDA assigns a point to the class whose mean is closest in terms of [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) while also accounting for the class prior probabilities.\n",
        "\n",
        "scikit-learn discusses this [here](https://scikit-learn.org/stable/modules/lda_qda.html#lda-qda-math), and more details can be found in Hastie et al. \"The Elements of Statistical Learning\" (2008).\n",
        "\n",
        "From Bayes' Law we have (note this assumes $x$ is a row vector; if column format is used the transposes below will switch around):\n",
        "\n",
        "$$P(y=k|x) = \\frac{P(x|y=k)P(y=k)}{P(x)} = \\frac{P(x|y=k)P(y=k)}{\\sum_l P(x|y=l)P(y=l)} \\sim P(x|y=k)P(y=k)$$\n",
        "\n",
        "The denominator is a constant which can be disregarded. In LDA (and [QDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html)) one simply chooses to model $P(x|y)$ as a multivariate Gaussian as follows:\n",
        "\n",
        "$$P(x|y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} {\\rm exp}\\left( -\\frac{1}{2}(x-\\mu_k)\\Sigma_k^{-1}(x-\\mu_k)^T\\right)$$\n",
        "\n",
        "where $\\Sigma_k$ is the covariance matrix of class $k$ and $\\mu_k$ is mean vector of class $k$; $d$ is the original dimensionality of $x$. The argument of the exponent is the [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance). QDA uses the covariance matrices of each class, however, if we assume they are the same ([homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity) assumption), then we can write:\n",
        "\n",
        "$${\\rm ln}P(y=k|x) = {\\rm ln}P(x|y=k) + {\\rm ln}P(y=k) + C = -\\frac{1}{2}(x-\\mu_k)\\Sigma^{-1}(x-\\mu_k)^T + {\\rm ln}P(y=k) + C'.$$\n",
        "\n",
        "Using the fact that the covariance matrix is [Hermitian](https://en.wikipedia.org/wiki/Hermitian_matrix), this can be simplified to:\n",
        "\n",
        "$$\n",
        "{\\rm ln}P(y=k|x) = -\\frac{1}{2} \\left[ x \\Sigma^{-1} x^T - 2x\\Sigma^{-1}\\mu_k^T + \\mu_k \\Sigma^{-1} \\mu_k^T \\right] + {\\rm ln}P(y=k) + C'.\n",
        "$$\n",
        "\n",
        "We are looking for the smallest distance, or maximum ${\\rm ln}P(y=k|x)$, across all classes; note the first term in the Mahalanobis distance expansion does not depend on $k$ so it is constant value across all $k$ expressions.  Thus, we can write:\n",
        "\n",
        "$$\n",
        "{\\rm ln}P(y=k|x) = \\overbrace{x\\Sigma^{-1}\\mu_k^T}^{{\\rm Linear~term}} -\\overbrace{\\frac{1}{2} \\mu_k \\Sigma^{-1} \\mu_k^T + {\\rm ln}P(y=k)}^{{\\rm k-dependent~ constant}} + C''.\n",
        "$$\n",
        "\n",
        "where $C''$ absorbs the constants from the Gaussian, denominator in Bayes' Law, and first term in the Mahalanobis distance expansion.  This can be rewritten as:\n",
        "\n",
        "$${\\rm ln}P(y=k|x) = \\omega_k^Tx^T + \\omega_{k,0} + C$$\n",
        "\n",
        "where $\\omega_k = \\Sigma^{-1}\\mu_k^T$ and $\\omega_{k,0} = -\\frac{1}{2}\\mu_k \\Sigma^{-1} \\mu_k^T + {\\rm ln}P(y=k)$ (in practice the $C''$ is neglected).  sklearn stores theses as `coef_` and `intercept_` in the LDA object.  The linear nature of this equation is why it is known as \"linear\" DA, vs. when we make no assumptions about $\\Sigma_k$ being equal for all classes, the result is quadratic (because the first term in the Mahalanobis distance expansion must be retained since it depends on $k$ now), hence QDA.\n",
        "\n",
        "Consequently, the above decision function can be computed (see `sklearn.discrimininant_analysis.LinearDiscriminantAnalysis.decision_function`) to provide the log probability of a given class.  Normalizing over all classes gives the `sklearn.discrimininant_analysis.LinearDiscriminantAnalysis.predict_proba` result, or the ability to predict the class probability.\n",
        "\n",
        "Estimating the covariance matrix\n",
        "---\n",
        "\n",
        "The assumption in LDA is that the covariance matrix is the same for each class, but how do we actually estimate this from our data?  In practice, this is done by a (de-biased) weighted average (i.e., the [pooled variance](https://en.wikipedia.org/wiki/Pooled_variance)) of each class' covariance matrix.\n",
        "\n",
        "$$\\Sigma = \\frac{\\sum_{i=1}^k(N_i-1)\\Sigma_k}{\\sum_{i=1}^k(N_i-1)}$$\n",
        "\n",
        "See this [stackexchange question](https://stats.stackexchange.com/questions/90615/estimating-the-covariance-matrix-in-linear-discriminant-analysis).  Note that sklearn can accommodate [other estimation methods](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis), though.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb44aad1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T12:25:10.679973Z",
          "start_time": "2023-09-19T12:25:10.646285Z"
        },
        "id": "fb44aad1"
      },
      "source": [
        "<h3>Implement the Algorithm</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce6a225",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:03.098245Z",
          "start_time": "2023-09-19T13:49:03.079117Z"
        },
        "id": "6ce6a225"
      },
      "outputs": [],
      "source": [
        "def lda_ln_prob(X_test, X_train, y_train):\n",
        "    \"\"\"Compute the lnP of classes based on LDA model of data.\"\"\"\n",
        "\n",
        "    # Assume that we are comparing to LDA using pre-scaled data\n",
        "    scaler = CorrectedScaler(with_mean=True, with_std=True)\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "    # Encode classes an integers and transform\n",
        "    enc = LabelEncoder()\n",
        "    y_train = enc.fit_transform(y_train.ravel())\n",
        "\n",
        "    # Identify the empirical means for each class\n",
        "    means = {}\n",
        "    for c in [enc.transform([class_])[0] for class_ in enc.classes_]:\n",
        "        mask = y_train == enc.inverse_transform([c])[0]\n",
        "        means[c] = np.mean(X_train[mask], axis=0)\n",
        "\n",
        "    # Compute pooled covariance estimate\n",
        "    sigma = np.zeros((X_train.shape[1], X_train.shape[1]))\n",
        "    for c in [enc.transform([class_])[0] for class_ in enc.classes_]:\n",
        "        mask = y_train == enc.inverse_transform([c])[0]\n",
        "        N = np.sum(mask)\n",
        "        sigma += (N-1)*np.cov(X_train[mask].T)\n",
        "    sigma /= (X_train.shape[0] - len(np.unique(y_train)))\n",
        "\n",
        "    # lda.coef_\n",
        "    coef = []\n",
        "    for c in [enc.transform([class_])[0] for class_ in enc.classes_]:\n",
        "        coef.append(np.matmul(np.linalg.inv(sigma), means[c].T))\n",
        "\n",
        "    # lda.intercept_\n",
        "    intercept = []\n",
        "    for c in [enc.transform([class_])[0] for class_ in enc.classes_]:\n",
        "        intercept.append(-0.5*np.matmul(np.matmul(means[c], np.linalg.inv(sigma)), means[c].T)+np.log(np.sum(y_train==enc.inverse_transform([c])[0])/X_train.shape[0]))\n",
        "\n",
        "    # Predict on test set\n",
        "    d = X_train.shape[1]\n",
        "    probs = np.zeros((X_test.shape[0], len(enc.classes_)))\n",
        "    for i,x in enumerate(scaler.transform(X_test)):\n",
        "        for j in [enc.transform([class_])[0] for class_ in enc.classes_]:\n",
        "            probs[i,j] = np.dot(coef[j], x) + intercept[j] # Constants related to Gaussian prefactor are ignored in sklearn\n",
        "\n",
        "    return probs, coef, intercept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94e5185d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:03.131402Z",
          "start_time": "2023-09-19T13:49:03.099892Z"
        },
        "id": "94e5185d"
      },
      "outputs": [],
      "source": [
        "# Let's make some sample data\n",
        "X, Y = sklearn.datasets.make_blobs(\n",
        "    n_samples=100,\n",
        "    n_features=2,\n",
        "    centers=3,\n",
        "    cluster_std=.5,\n",
        "    shuffle=True,\n",
        "    random_state=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc5a63e3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:03.478539Z",
          "start_time": "2023-09-19T13:49:03.133086Z"
        },
        "id": "cc5a63e3"
      },
      "outputs": [],
      "source": [
        "for class_ in np.unique(Y):\n",
        "    X_ = X[Y == class_]\n",
        "    plt.scatter(X_[:,0], X_[:,1], label=f'Class {class_}')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(r'$X_1$')\n",
        "_ = plt.ylabel(r'$X_2$')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0a160d7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:03.496921Z",
          "start_time": "2023-09-19T13:49:03.480182Z"
        },
        "id": "a0a160d7"
      },
      "outputs": [],
      "source": [
        "# For this example we will use the entire dataset for training\n",
        "scaler = CorrectedScaler()\n",
        "lda = LDA(n_components=1, store_covariance=True)\n",
        "_ = lda.fit(scaler.fit_transform(X), Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7c0054",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:03.563807Z",
          "start_time": "2023-09-19T13:49:03.498525Z"
        },
        "id": "af7c0054"
      },
      "outputs": [],
      "source": [
        "my_ln_probs, coef, intercept = lda_ln_prob(X, X, Y.reshape(-1,1))\n",
        "sklearn_ln_probs = lda.decision_function(scaler.transform(X))\n",
        "\n",
        "# To prevent overflow\n",
        "my_shifted_probas = (my_ln_probs.T - np.max(my_ln_probs, axis=1)).T\n",
        "\n",
        "my_probas = (np.exp(my_shifted_probas).T/np.sum(np.exp(my_shifted_probas), axis=1)).T\n",
        "sklearn_probas = lda.predict_proba(scaler.transform(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f0b267",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:03.579810Z",
          "start_time": "2023-09-19T13:49:03.565362Z"
        },
        "id": "d2f0b267"
      },
      "outputs": [],
      "source": [
        "assert np.allclose(my_ln_probs, sklearn_ln_probs) # Manual decision function agrees with sklearn\n",
        "assert np.allclose(my_probas, sklearn_probas) # Manual class probability agrees with sklearn\n",
        "assert np.allclose(lda.coef_, coef) # Manual coef agrees with sklearn\n",
        "assert np.allclose(lda.intercept_, intercept) # Manual intercept agrees with sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68519632",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:03.908674Z",
          "start_time": "2023-09-19T13:49:03.582859Z"
        },
        "id": "68519632"
      },
      "outputs": [],
      "source": [
        "disp = DecisionBoundaryDisplay.from_estimator(lda, scaler.transform(X))\n",
        "\n",
        "for class_ in np.unique(Y):\n",
        "    X_ = scaler.transform(X)[Y == class_]\n",
        "    disp.ax_.scatter(X_[:,0], X_[:,1], label=f'Class {class_}')\n",
        "disp.ax_.legend(loc='best')\n",
        "disp.ax_.set_xlabel(r'$X_1$')\n",
        "_ = disp.ax_.set_ylabel(r'$X_2$')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82ff5144",
      "metadata": {
        "id": "82ff5144"
      },
      "source": [
        "<h3>Impact of Class Imbalance</h3>\n",
        "\n",
        "Observe that the constant in the discriminant function involves a term ${\\rm ln}P(y=k)$.  This comes from the prior in the first equation (Bayes' Law).  This is absorbed in the `intercept_` and may be relative weak compared to other terms, but it is nonetheless a factor in determining the boundary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34e2f66d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:03.929012Z",
          "start_time": "2023-09-19T13:49:03.910363Z"
        },
        "id": "34e2f66d"
      },
      "outputs": [],
      "source": [
        "def make_imblance(factor=10):\n",
        "    X, Y = sklearn.datasets.make_blobs(\n",
        "        n_samples=500,\n",
        "        n_features=2,\n",
        "        centers=2,\n",
        "        cluster_std=1,\n",
        "        shuffle=True,\n",
        "        random_state=0\n",
        "    )\n",
        "\n",
        "    # Let's remove a fraction of class 0 relative to the others\n",
        "    mask = Y == 0\n",
        "\n",
        "    X_ = X[~mask]\n",
        "    Y_ = Y[~mask]\n",
        "\n",
        "    X_ = np.vstack((X_, X[mask][:np.sum(mask)//factor]))\n",
        "    Y_ = np.concatenate((Y_, Y[mask][:np.sum(mask)//factor]))\n",
        "\n",
        "    return X_, Y_\n",
        "\n",
        "def visualize(lda, scaler, X, Y, ax):\n",
        "    disp = DecisionBoundaryDisplay.from_estimator(lda, X, ax=ax, response_method='predict')\n",
        "\n",
        "    for class_ in np.unique(Y):\n",
        "        X_ = X[Y == class_]\n",
        "        disp.ax_.scatter(X_[:,0], X_[:,1], label=f'Class {class_}')\n",
        "    disp.ax_.legend(loc='best')\n",
        "    disp.ax_.set_xlabel(r'$X_1$')\n",
        "    disp.ax_.set_ylabel(r'$X_2$')\n",
        "    disp.ax_.set_xlim(-1.5, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "624bb9a0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:04.486398Z",
          "start_time": "2023-09-19T13:49:03.930512Z"
        },
        "id": "624bb9a0"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
        "\n",
        "# Balanced\n",
        "X_, Y_ = make_imblance(factor=1)\n",
        "scaler = CorrectedScaler()\n",
        "lda = LDA(n_components=1, store_covariance=True)\n",
        "_ = lda.fit(X_, Y_)\n",
        "\n",
        "visualize(lda, scaler, X_, Y_, ax=axes[0])\n",
        "\n",
        "# Imbalanced\n",
        "X_, Y_ = make_imblance(factor=10)\n",
        "scaler = CorrectedScaler()\n",
        "lda = LDA(n_components=1, store_covariance=True)\n",
        "_ = lda.fit(X_, Y_)\n",
        "\n",
        "visualize(lda, scaler, X_, Y_, ax=axes[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceff079d",
      "metadata": {
        "id": "ceff079d"
      },
      "source": [
        "LDA as a dimensionality reduction technique\n",
        "---\n",
        "\n",
        "Interestingly, the LDA model also can function as a dimensionality reduction tool.  In fact, this is how it was [originally derived](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) by Sir Ronald Fisher in 1936 in a slightly form often called \"Fisher's linear discriminant.\"  Today, the terms are used interchangeably.\n",
        "\n",
        "If 2 classes have means, $\\mu_0$ and $\\mu_1$, and covariances, $\\Sigma_0$ and $\\Sigma_1$, then the linear combination of features $\\vec{w} \\cdot \\vec{x}$ will have mean $\\vec{w} \\cdot \\mu_i$ and variance $\\vec{w} \\Sigma_i \\vec{w}^T$ for each class, $i$. Fisher defined the separation between these distributions as the ratio of the variance between the classes to the variance within the classes:\n",
        "\n",
        "$$\n",
        "S = \\frac{\\sigma_{{\\rm between}}^2}{\\sigma_{{\\rm within}}^2} = \\frac{\\left(\\vec{w} \\cdot \\mu_1 - \\vec{w} \\cdot \\mu_0\\right)^2}{\\vec{w} \\Sigma_0 \\vec{w}^T + \\vec{w} \\Sigma_1 \\vec{w}^T} = \\frac{\\left(\\vec{w} \\cdot \\left( \\mu_1 - \\mu_0 \\right) \\right)^2}{\\vec{w} \\left( \\Sigma_0 + \\Sigma_1 \\right) \\vec{w}^T}\n",
        "$$\n",
        "\n",
        "Maximum separation occurs when:\n",
        "\n",
        "$$\n",
        "\\vec{w} \\sim \\left( \\Sigma_0 + \\Sigma_1 \\right)^{-1} \\left( \\mu_1 - \\mu_0 \\right)\n",
        "$$\n",
        "\n",
        "When classes are normally distributed and have equal covariances (LDA classifier assumptions) this results in the same equations above.\n",
        "\n",
        "According to [Wikipedia](https://en.wikipedia.org/wiki/Linear_discriminant_analysis):\n",
        "    \n",
        "> \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification. \"\n",
        "\n",
        "Sebastian Raschka has a nice discussion [here](https://sebastianraschka.com/Articles/2014_python_lda.html) with illustrations in python.\n",
        "\n",
        "> \"It should be mentioned that LDA assumes normal distributed data, features that are statistically independent, and identical covariance matrices for every class. However, this only applies for LDA as classifier and LDA for dimensionality reduction can also work reasonably well if those assumptions are violated.\"\n",
        "\n",
        "![image](https://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png)\n",
        "\n",
        "With [PCA](pca_pcr.ipynb), we found the orthogonal principal components that characterized the spread of the data, i.e., the covariance of X with itself (unsupervised).  With [PLS](pls.ipynb), we looked for directions that characterized the covariance of the product of X and Y (supervised).  LDA is a supervised method which instead looks for axes that **maximize the separation between labelled classes**.  This is done by finding the eigenvectors (\"linear discriminants\") of the matrix $S_W^{-1}S_B$, where $S_W$ is the within-class scatter matrix and $S_B$ is the between-class scatter matrix.  LDA for dimensionality reduction can be described in 5 steps:\n",
        "\n",
        "1. Compute the $p$-dimensional mean vectors for all classes from the dataset.\n",
        "\n",
        "2. Compute the scatter matrices (between-class and within-class scatter matrix).\n",
        "\n",
        "3. Compute the eigenvectors and eigenvalues for these matrices.\n",
        "\n",
        "4. Sort the eigenvectors by decreasing eigenvalue and choose the first $k$ eigenvectors; stack these columns to form a $p \\times k$ dimensional matrix $W$.  This is analogous to the \"loadings\" matrix in PCA.\n",
        "\n",
        "5. Use this $p \\times k$ matrix to project the samples into the new subspace by performing matrix multiplication: $T = XW$, where $T$ are the \"x-scores\".\n",
        "\n",
        "PCA can be used to perform dimensionality reduction by only selecting the leading $k$ eigenvectors from the covariance matrix; assuming it is full rank, we have as many dimensions as the size of that matrix.  However, in LDA, the matrix $S_W^{-1}S_B$ will only have [at most](https://en.wikipedia.org/wiki/Linear_discriminant_analysis#Multiclass_LDA) ${\\rm min}(p, c-1)$ non-zero eigenvectors where $p$ is the number of features and $c$ is the number of classes.  Thus, if we want to separate 2 classes, LDA will only be able to return the single axes that separates them best.  In such a case, it might be better to even do PCA if we desire a low, but higher than 1-, dimensional result.\n",
        "\n",
        "In general though, the **intuitive explanation for LDA is that it finds the orthogonal axes that best separate classes**.  Thus, it is a very useful dimensionality reduction approach for classification.  It is particularly helpful in identifying a subspace that is highly discriminative, in which other models can be used to compute decision boundaries.  For example, a pipeline might look like:\n",
        "\n",
        "1. Standardize data\n",
        "2. LDA to project into a small number of dimensions\n",
        "3. Use logistic regression to classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23528a8e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:04.506535Z",
          "start_time": "2023-09-19T13:49:04.487823Z"
        },
        "id": "23528a8e"
      },
      "outputs": [],
      "source": [
        "data = sklearn.datasets.load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, shuffle=True, train_size=0.8, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538da46f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:04.561515Z",
          "start_time": "2023-09-19T13:49:04.508202Z"
        },
        "id": "538da46f"
      },
      "outputs": [],
      "source": [
        "# Scale and perform LDA to project into 2D\n",
        "scaler = CorrectedScaler()\n",
        "lda = LDA(n_components=2)\n",
        "X_train_std_lda = lda.fit_transform(scaler.fit_transform(X_train), y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In LDA this is called the \"discriminability\" ratio and is the ratio of the\n",
        "# eigenvalues, just like in PCA\n",
        "lda.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "DW0mjD9llmQT"
      },
      "id": "DW0mjD9llmQT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare this to PCA in the same number of dimensions\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2).fit(scaler.transform(X_train))\n",
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "RNK_tO_Unj3W"
      },
      "id": "RNK_tO_Unj3W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The \"transform\" member does the matrix multiplication to obtain the scores\n",
        "np.allclose(\n",
        "    X_train_std_lda,\n",
        "    np.matmul(scaler.transform(X_train), lda.scalings_)\n",
        ")"
      ],
      "metadata": {
        "id": "3tA9k4u6h2qr"
      },
      "id": "3tA9k4u6h2qr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The \"W\" matrix is stored as the \"scalings_\" member of the LDA object.\n",
        "# Recall, these are the coefficients on each feature for each eigenvector\n",
        "# we have computed.\n",
        "\n",
        "lda.scalings_"
      ],
      "metadata": {
        "id": "EZobr3SyiX74"
      },
      "id": "EZobr3SyiX74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is the analogy in PCA.  Clearly these two methods have found different\n",
        "# vectors which \"summarize\" the data!\n",
        "pca.components_.T"
      ],
      "metadata": {
        "id": "_713Yg8JpRfd"
      },
      "id": "_713Yg8JpRfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can build the analogy of a loadings plot in PCA by using the scalings.\n",
        "# This allows to interpret which features are the most significant in terms\n",
        "# of what features SEPARATE the classes.\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
        "\n",
        "for name, pos in zip(data['feature_names'], lda.scalings_):\n",
        "    axes[0].plot(pos[0], pos[1], 'o', color='C0', label=name)\n",
        "    axes[0].text(pos[0], pos[1], name)\n",
        "axes[0].axvline(0, ls='--', color='k')\n",
        "axes[0].axhline(0, ls='--', color='k')\n",
        "\n",
        "axes[0].set_xlabel(f'LD 1 ({\"%.3f\"%(lda.explained_variance_ratio_[0]*100)}%)')\n",
        "axes[0].set_ylabel(f'LD 2 ({\"%.3f\"%(lda.explained_variance_ratio_[1]*100)}%)')\n",
        "axes[0].set_title('LDA')\n",
        "\n",
        "for name, pos in zip(data['feature_names'], pca.components_.T):\n",
        "    axes[1].plot(pos[0], pos[1], 'o', color='C0', label=name)\n",
        "    axes[1].text(pos[0], pos[1], name)\n",
        "axes[1].axvline(0, ls='--', color='k')\n",
        "axes[1].axhline(0, ls='--', color='k')\n",
        "\n",
        "axes[1].set_xlabel(f'PC 1 ({\"%.3f\"%(pca.explained_variance_ratio_[0]*100)}%)')\n",
        "axes[1].set_ylabel(f'PC 2 ({\"%.3f\"%(pca.explained_variance_ratio_[1]*100)}%)')\n",
        "axes[1].set_title('PCA')"
      ],
      "metadata": {
        "id": "5GnbCYNCoDP2"
      },
      "id": "5GnbCYNCoDP2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2371a138",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:04.637330Z",
          "start_time": "2023-09-19T13:49:04.563163Z"
        },
        "id": "2371a138"
      },
      "outputs": [],
      "source": [
        "# Train a LR classifier in LDA's 2D space\n",
        "lr = LogisticRegression(C=1.0, random_state=0)\n",
        "_ = lr.fit(X_train_std_lda, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a8c620f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:05.193069Z",
          "start_time": "2023-09-19T13:49:04.640008Z"
        },
        "id": "7a8c620f"
      },
      "outputs": [],
      "source": [
        "disp = DecisionBoundaryDisplay.from_estimator(lr, X_train_std_lda, response_method='predict', grid_resolution=1000)\n",
        "\n",
        "for class_ in np.unique(y_train):\n",
        "    X_ = X_train_std_lda[y_train == class_]\n",
        "    disp.ax_.scatter(X_[:,0], X_[:,1], label=f'Class {class_}')\n",
        "disp.ax_.legend(loc='best')\n",
        "disp.ax_.set_xlabel(r'LD 1')\n",
        "disp.ax_.set_ylabel(r'LD 2')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d42e624",
      "metadata": {
        "id": "1d42e624"
      },
      "source": [
        "You can see that the decision boundaries along the first component axis (x-axis) are **nearly vertical** - this is because LDA finds the axes that best separate the classes from each other as you move along it.  The data here is particularly amenable to this approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57a122f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:06.806761Z",
          "start_time": "2023-09-19T13:49:05.194912Z"
        },
        "id": "f57a122f"
      },
      "outputs": [],
      "source": [
        "pipe = Pipeline([\n",
        "    ('scaler', CorrectedScaler()),\n",
        "    ('lda', LDA()),\n",
        "    ('lr', LogisticRegression(random_state=0))\n",
        "]\n",
        ")\n",
        "\n",
        "grid = [\n",
        "    {'lda__n_components':[1,2], # Limited to n_classes-1\n",
        "     'lr__C':np.logspace(-5,0,10),\n",
        "    }\n",
        "]\n",
        "\n",
        "gs = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=grid,\n",
        "    refit=True,\n",
        "    cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=0),\n",
        "    n_jobs=-1\n",
        ")\n",
        "_ = gs.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "152656ad",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:06.841161Z",
          "start_time": "2023-09-19T13:49:06.810113Z"
        },
        "id": "152656ad"
      },
      "outputs": [],
      "source": [
        "gs.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81392441",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-19T13:49:06.866414Z",
          "start_time": "2023-09-19T13:49:06.843345Z"
        },
        "id": "81392441"
      },
      "outputs": [],
      "source": [
        "gs.score(X_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}